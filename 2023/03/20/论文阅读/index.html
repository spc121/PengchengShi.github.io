


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  论文阅读 |    Yiyang Ai Lab.</title>
  <meta name="description" content="Personal website. Recording some creative things">
  <!-- 标签页图标 -->
  
  <link rel="shortcut icon" href="https://github.com/YiyangZhou/imageBeds/blob/main/imgs/collage-line.png" type="image/x-icon">
  

  <!-- 图标库 -->
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <!-- 动画库 -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  
  <!-- css文件 -->
  
<link rel="stylesheet" href="/css/white.css">

  <!-- 代码高亮 -->
  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>

<div class="menu-outer">
    <div class="menu-inner">
      <div class="menu-site-name  animate__animated  animate__fadeInUp">
        <a href="/">
          Yiyang Ai Lab.
        </a>
        
      </div>
      <div class="menu-group">
        <ul class="menu-ul">
        
          <a href="/" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              HOME
            </li>
          </a>
        
          <a href="/archives" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              BLOG
            </li>
          </a>
        
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu">
            <i class="ri-menu-line"></i>
          </li>
        
        </ul>

      </div>

    </div>
</div>
<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp">
      <a href="/">
        Yiyang Ai Lab.
      </a>
    </div>
    <div class="mobile-menu-group" id="mobile-close">
      <i class="ri-close-line"></i>
    </div>

  </div>

  <div class="mobile-menu-div">
  
    <a href="/" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>HOME</span>
      </div>
    </a>
  
    <a href="/archives" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>BLOG</span>
      </div>
    </a>
  
  
  </div>


</div>

<div class="body-outer">
  <div class="body-inner">
    
<article class="post-inner">
  <div class="post-content-outer">
    <div class="post-intro">
      <div class="post-title animate__animated  animate__fadeInUp">论文阅读</div>
      <div class="meta-intro animate__animated  animate__fadeInUp">Mar 20 2023</div>
      
    </div>
    <div class="post-content-inner">
      <div class="post-content-inner-space">

      </div>
      <div class="post-content-main animate__animated  animate__fadeInUp">
        <!-- top型目录 -->
        
        <h1 id="ChatGPT相关"><a href="#ChatGPT相关" class="headerlink" title="ChatGPT相关"></a>ChatGPT相关</h1><h2 id="1-相关文献总结"><a href="#1-相关文献总结" class="headerlink" title="1.相关文献总结"></a>1.相关文献总结</h2><p><strong>（1）多模态预训练：</strong></p>
<p><strong>综述</strong>：</p>
<p>​        Vision-Language Pre-training: Basics, Recent Advances, and Future Trends</p>
<p><strong>多模态框架（包含：ALBEF, BLIP, BLIP2等)</strong>: <a target="_blank" rel="noopener" href="https://github.com/salesforce/LAVIS">https://github.com/salesforce/LAVIS</a>  </p>
<p><strong>博文推荐</strong>：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/616351346">https://zhuanlan.zhihu.com/p/616351346</a></p>
<hr>
<p><strong>（2）数据集：</strong></p>
<p><strong>图文</strong>: </p>
<p>​        MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation (MMDialog) [✅]</p>
<p><strong>视频</strong></p>
<p>​        TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat (TikTalk) [✅]</p>
<hr>
<p><strong>（3）纯文本ChatGPT：</strong></p>
<p><strong>大模型</strong></p>
<p>​        Improving Language Understanding By Genertative Pre-trying (GPT-1) [✅]</p>
<p>​        Language Models are Unsupervised Multitask Learners (GPT-2) [✅]</p>
<p>​        Language Models are Few-Shot Learners (GPT-3) [✅]</p>
<p>​        Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>
<p><strong>Instruction tuning</strong></p>
<p>​        Finetuned Language Models Are Zero-Shot Learners (FLAN) [✅]</p>
<p>​        Multitask Prompted Training Enables Zero-Shot Task Generalization (T0) [✅]</p>
<p>​        Scaling Instruction-Finetuned Language Models (Flan-PaLM) [✅]</p>
<p>​        SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions (SELF-INSTRUCT) [✅]</p>
<p><strong>大模型+对话</strong></p>
<p>​        Training language models to follow instructions with human feedback (InstructGPT) [✅]</p>
<p>​        Baize: An Open-Source Chat Model with Parameter-Effificient Tuning on Self-Chat Data (Baize) [✅]</p>
<hr>
<p><strong>（4）多模态ChatGPT：</strong></p>
<p>​        Flamingo: a Visual Language Model for Few-Shot Learning (Flamingo) [✅]</p>
<p>​        BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Blip-2) [✅]</p>
<p>​        GIT: A Generative Image-to-text Transformer for Vision and Language</p>
<p>​        MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning </p>
<p>​        Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering (Prophet) [✅]</p>
<p>​        Language Is Not All You Need: Aligning Perception with Language Models (KOSMOS-1) [✅]</p>
<p>​        Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models (Visual ChatGPT) [✅]</p>
<p>​        MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action (MM-REACT) [✅]</p>
<p>​        REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering (REVIVE) [✅]</p>
<p>​        An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA (PICa) [✅]</p>
<p>​        MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (MultiInstruct) [✅]</p>
<p>​        MiniGPT-4:Enhancing Vision-Language Understanding with Advanced Large Language Models (MiniGPT-4) [✅]</p>
<p>​        Visual Instruction Tuning (LLaVa) [✅]</p>
<p><strong>（5）评测：</strong></p>
<p>​        《txt2img的评测》TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering (TIFA) [✅]</p>
<h2 id="2-文献阅读笔记"><a href="#2-文献阅读笔记" class="headerlink" title="2.文献阅读笔记"></a>2.文献阅读笔记</h2><h3 id="2-1-Prophet"><a href="#2-1-Prophet" class="headerlink" title="2.1.Prophet"></a>2.1.Prophet</h3><blockquote>
<p>本文指出早期用外部知识库来引导回答图片问题KB-VQA任务，外部知识的一些无关信息可能会造成误导。本文提出了用VQA模型生成一系列针对图片的指导信息，然后用这些构造成prompt去指导文本GPT生成答案。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>这部分指出传统的KB-VQA任务会存在两个问题：</p>
<p>（1）所需的知识可能无法从kb中成功检索</p>
<p>（2）即使检索到所需的知识，也不可避免地引入大量无关的知识</p>
<p>但是也会有像PICa这类模型用caption model去获取图片的caption，然后用得到的caption和一些固定格式的promt输入GPT-3去得到答案。</p>
<blockquote>
<p>本文说上述两种方法都有缺陷：</p>
<p>（1）PICa的方法用的caption model可能并不会得到关于问题的信息：</p>
<p>eg.</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230320170146043.png" >
        </sapn>
      </p>
<p>这张图中caption并不会注意到椰子树，如果用这个caption组合，然后问：图中的树会长什么果子？这样会有问题。</p>
<p>（2）在给到一个任务给GPT-3时候，它依赖上下文，所以上下文例子选择至关重要。</p>
</blockquote>
<p>Prophet的上下文策略文中叫做answer heuristics，有两种形式：1.answer candidates. 2.answer-aware examples.</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230320171429176.png" >
        </sapn>
      </p>
<p><strong>Framework</strong></p>
<p>整个模型是两阶段</p>
<p>（1）第一阶段：训练一个VQA模型获得两种形式的answer heuristics</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230324163841914.png" >
        </sapn>
      </p>
<p>​        其中answer-aware examples的选择是通过上述式子计算出融合模态表征同其他样本融合表征的相似度，因为融合后的表征相似度高，他们所在的潜在答案空间重叠就高，然后选取相似度最高的N个做为上下文answer-aware examples。</p>
<p>（2）第二阶段：在启发式增强的提示阶段，answer heuristics和caption被集成到一个格式化的提示中，以指导GPT-3预测一个答案</p>
<p>整个模型如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230320172053320.png" >
        </sapn>
      </p>
<h3 id="2-2-MM-React"><a href="#2-2-MM-React" class="headerlink" title="2.2.MM-React"></a>2.2.MM-React</h3><blockquote>
<p>MM-React的prompt设计利于LLM接受多模态信息。本文注重于利用单独的视觉模型和LLM分离的让LLM理解图像。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>这部分指出：</p>
<p>​    很多工作是多模态融合，类似于Flamingo和PaLM-E等，这种模型效果好但计算贵。</p>
<p>MM-React输入图片等非文本模态的数据的使用直接采用PATH，相当于告诉语言模型这有个占位符。</p>
<p><strong>Framework</strong></p>
<p>不同的视觉模型提供不同的信息，例如：目标检测，OCR，实体命名模型。MM-React的目的是根据用户以自然语言查询提供的需求来自动化这个过程。</p>
<p>整个框架如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230327110743308.png" >
        </sapn>
      </p>
<p>gpt在系统中的一个思考流程：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230327112631996.png" >
        </sapn>
      </p>
<h3 id="2-3-Self-Instruct"><a href="#2-3-Self-Instruct" class="headerlink" title="2.3.Self-Instruct"></a>2.3.Self-Instruct</h3><blockquote>
<p>现在的instruction-tuned llm能力不错，但是人工标注费时费力且质量多样性不高，所以本文提出self-instruct，让模型自己指导自己遵循指令。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>人工撰写instruct数据集缺乏多样性，偏向于热门的NLP任务，没有涵盖真正的各种任务和不同的方式来描述它们。考虑到这些限制，要继续提高指令调优模型的质量，就需要进行开发替代方法。</p>
<p>self-instruct的整个流程是一个自我提高算法，从一个人工的instruction小数据集开始，指导自己整个生成过程。在第一阶段中，将提示模型为新任务生成指令。这一步利用现有的指令集合来创建更广泛的指令来定义( 通常是新的)任务。给定新生成的指令集，该框架还为它们创建输入-输出实例，稍后可以用于监督指令调优。最后，用各种措施来删除低质量和重复的指令。这个过程可以重复进行许多交互，直到完成大量的任务。</p>
<p><strong>Framework</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330104229062.png" >
        </sapn>
      </p>
<p><strong>Limitation</strong></p>
<p>(1) Tail Phenomena</p>
<p>(2) Dependence on large models</p>
<p>(3) Reinforcing LM biases</p>
<h3 id="2-4-REVIVE"><a href="#2-4-REVIVE" class="headerlink" title="2.4.REVIVE"></a>2.4.REVIVE</h3><blockquote>
<p>本文提出现有的很多kbVQA方法存在两个问题（1）关注整个视觉，或者滑动窗口检索，忽视了region对于vqa任务的重要性（2）没有很好利用视觉模态特征。本文提出的REVIVE说明了这些region信息的重要性，主要利用显示的检索信息和region信息。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>现有工作聚焦于整合外部knowledge，本文侧重于视觉一侧聚焦region对于vqa任务的重要性。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330142210728.png" >
        </sapn>
      </p>
<p><strong>Framework</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330142645321.png" >
        </sapn>
      </p>
<p>整体来说首先搜集了四种信息：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330144541446.png" >
        </sapn>
      </p>
<p>region视觉信息，box信息描述方位，和box的tag信息，以及caption用于描述物体的关系（感觉很有道理这个caption）。</p>
<p>除了正常的kb知识库Q,本文为region区域也建立了知识库，具体做法为：</p>
<p>（1）reformat原始的知识库，构建成实体和描述</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330145745769.png" >
        </sapn>
      </p>
<p>（2）检索方式类似于正常的kb知识库</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330145902576.png" >
        </sapn>
      </p>
<h3 id="2-5-MultiInstruct"><a href="#2-5-MultiInstruct" class="headerlink" title="2.5.MultiInstruct"></a>2.5.MultiInstruct</h3><blockquote>
<p>在这项工作中，作者介绍了多指令，第一个由多模态指令调整基准数据集组成的多模态指令调整基准数据集，模式任务涵盖11个大类别。每个任务至少设计了5000个来自现有开源数据集的实例（输入输出对）和5个专家编写的指令。文中还设计了一个新的评估度量-敏感度，以评估模型对各种指令的敏感性。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>每条实例被组织成统一的序列到序列格式，其中输入文本、图像、指令和边界框在相同的标记空间中表示。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230403100351129.png" >
        </sapn>
      </p>
<p><strong>MultiInstruct</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230403101401642.png" >
        </sapn>
      </p>
<h3 id="2-6-MiniGPT-4"><a href="#2-6-MiniGPT-4" class="headerlink" title="2.6.MiniGPT-4"></a>2.6.MiniGPT-4</h3><blockquote>
<p>本文认为gpt4得益于使用了更先进的语言模型，本文提出了MiniGPT-4，它使用一个投影层将一个冻结的视觉编码器与一个冻结的LLM（Vicuna）对齐。整个模型只用了大约500万对图像-文本对和额外的3500个精选的高质量对进行投影层的训练。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>本文主要认为gpt4的超强能力都源于一个强大的LLM，文本端是Vicuna视觉端是ViT-G/14和Q-Former。MiniGPT-4只添加了一个投影层，以将编码的视觉特征与Vicuna语言模型对齐，并冻结了所有其他的视觉和语言组件。</p>
<p>MiniGPT-4先用基本的图文对数据来train，使得文本图像端能对齐，然后用3500高质量pairs+自定模版来激发能力。</p>
<p>本文证明了：</p>
<p>​    （1）我们的研究表明，通过将视觉特征与先进的大型语言模型Vicuna相结合，我们可以实现突发的视觉语言能力。我们证明了我们的MiniGPT-4可以推进 与GPT-4演示中展示的能力相似。</p>
<p>​    （2）仅仅训练一个投影层就可以有效地进行训练 y将视觉特征与大型语言模型对齐。</p>
<p>​    （3）公开数据质量低，不足以对齐，得加入一些超高质量数据。</p>
<p><strong>Framework</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230420134831112.png" >
        </sapn>
      </p>
<p>本文说图文对预训练（这里的预训练只训练mlp），minigpt4能有能力但是语言不完整，常常生成碎片化的重复的东西。我们还注意到，在GPT-3中也面临着类似的问题。尽管在广泛的语言数据集上进行了预训练，GPT-3仍然不能直接生成与用户的意图一致的语言输出，需要微调解决。</p>
<p>第一阶段：</p>
<p>图文对预训练，SBU和LAION</p>
<p>第二阶段：</p>
<p>用高质量数据集来微调，高质量图文对主要体现为文本的高质量</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230420135801654.png" >
        </sapn>
      </p>
<p>它采用prompt一次判断句子长度是否超出80tokens，如果超出则保存，否则加一个下面的prompt，把两次的句子连起来，这样得到5000个图文对。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230420141256875.png" >
        </sapn>
      </p>
<p>然后又人工筛选出里面质量最高的3500条。</p>
<p>然后根据格式：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230420141441097.png" >
        </sapn>
      </p>
<p>进行二阶段训练。</p>
<p><strong>Limitations</strong></p>
<p>感知能力不足。MiniGPT-4的视觉感知仍然有限。它可能难以从图像中识别详细的文本信息，并区分空间定位。可能源于以下几个因素：</p>
<p>1)缺乏足够的对齐图像-文本数据，其中包含足够的信息，如空间定位和光学字符注释。这个问题可以通过对更一致和丰富的数据的训练来缓解</p>
<p>2)还有就是说整体匹配的视觉端就像Q-fomer和clip的vit这种会丢失很多视觉信息</p>
<p>3)只训练一个投影层可能无法提供足够的能力来学习广泛的视觉-文本对齐</p>

        <!-- 分类文章 -->
        
      </div>
      <div class="post-content-inner-space">
        
          <div class="space-toc-main animate__animated  animate__fadeInUp">
            <ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE%E6%80%BB%E7%BB%93"><span class="space-toc-text">1.相关文献总结</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#2-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0"><span class="space-toc-text">2.文献阅读笔记</span></a></li></ol>
           </div>
        
      </div>
   </div>
    <!-- 评论 -->
    
  </div>
</article>
  </div>
</div>



<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->

  <div class="footer-outer animate__animated  animate__fadeInUp">
    <div class="footer-inner">
    <div class="footer-text">
    <p>Power by <a target="_blank" rel="noopener" href="http://hexo.io/">Hexo</a> Theme by <a target="_blank" rel="noopener" href="https://github.com/FuShaoLei/hexo-theme-white">White</a></p>

    </div>
    <div class="footer-contact">
    <ul class="footer-ul">
        
        <li class="footer-li">
            <a href="https://github.com/YiyangZhou" target="_blank">
                <i class="ri-github-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="mailto:zhouyiyangailab@gmail.com" target="_blank">
                <i class="ri-mail-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="https://scholar.google.com/citations?user=6KltFMAAAAAJ&amp;hl=zh-CN" target="_blank">
                <i class="ri-google-fill"></i>
            </a>
        </li>
        
    </ul>
    </div>
    </div>
</div>






<script src="/js/white.js"></script>



</body>
</html>
